{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Question : 1** \n",
    "\n",
    "\n",
    " **Linear Least Squares** **[5 points]**\n",
    "\n",
    "\n",
    "The simplest model to predict the spread of infectious diseases is the SIR model. This model is a set of ordinary differential equations that describe the evolution of the number of susceptible (S(t)), infected (I(t)) and recovered/removed (R(t)) populations in a closed system. The equations are\n",
    "\n",
    "$\\frac{dS}{dt} = \\frac {−βIS} {N} $, (1)\n",
    "\n",
    "$\\frac{dI}{dt} =\\frac {βIS} {N} −γI$, (2)\n",
    "\n",
    "$\\frac{dR}{dt} = γI$, (3)\n",
    "\n",
    "where $N = S(t)+I(t)+R(t)$. The basic reproduction number $R_0 = β/γ$ is defined to quantify the new infections that one infected person causes and is considered as a magic number to identify if an infectious disease is under control. For example, if $R_0 > 1$, the disease has an exponential growth whereas if $R_0 < 1$, the disease is under control and the infectious population will eventually go to zero. At peak $R_0$ will cross 1.\n",
    "As with simple models, there exist analytical solutions to the SIR model. One form of the solution is given as\n",
    "\n",
    "$S(t) = S(0) exp(−χ(t))$ , (4)\n",
    "\n",
    "$I(t) = N − S(t) − R(t)$ , (5)\n",
    "\n",
    "$R(t) = R(0) + ρχ(t)$ , (6)\n",
    "\n",
    "$χ(t) = \\frac {β} {N} \\int_{0}^{t}I(t^*) dt^*$. (7)\n",
    "\n",
    "\n",
    "For this assignment, we will consider a time unit of days, total population of India as 130 crores and the time horizon of interest as March 23, 2020 to Oct 15, 2020. Removed is a sum of recovered and deceased, i.e., the population that will not get infected again. Data in CSV form for state-wise-daily is available at\n",
    "https://data.covid19india.org/csv/latest/state_wise_daily.csv\n",
    "\n",
    "1. Download the state wise daily data of infected, recovered and deceased from the covid19india website. The data gives daily new infections, recovery and deceased. Use Pandas and create time-series of all India $I(t), S(t)$ and $R(t)$. Plot these time- series. Hint: Apply yourself and see what $I(t)$ means and what the data provides.\n",
    "2. Formulate the problem of estimating $γ$ and $β$ as a linear least squares problem.\n",
    "3. Form the Jacobian matrix and calculate its rank and condition number.\n",
    "4. Form the coefficient matrix and calculate its condition number. Find the relation between this condition number and condition number of the Jacobian.\n",
    "5. Code the cholesky factorization approach to solve the linear least squares problem.\n",
    "6. Apply your code and estimate $β, γ$ and $R_0$.\n",
    "7. Use scipy.optimize and estimate $β, γ$ and $R_0$.\n",
    "8. State your observations in the above two items and give reasons.\n",
    "9. Estimate $R_0(t)$ as a function of time by utilizing data until t to estimate the different parameters. Plot $R_0(t)$. \n",
    "10. Based on the above analyse the state of the pandemic in India. Has the peak passed as on Oct 2020?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "POPULATION = 130 * (10**7)\n",
    "\n",
    "df = pd.read_csv('./data/state_wise_daily.csv')\n",
    "summed_data = df.sum(axis=1,numeric_only=True)   # Sum up along rows\n",
    "summed_data = summed_data.loc[27:648].reset_index(drop=True) # Only use the data from March 23 to October 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formula Used\n",
    "\n",
    "- S(1) = Initial_population - Infected count from csv on day 1\n",
    "- R(1) = (Recovered +Deceased) count from csv on day 1\n",
    "- I(1) = Infected count from csv on day 1 - R(1)\n",
    "\n",
    "Further on\n",
    "- S(t) = S(t-1) - Infected count from csv on day t\n",
    "- R(t) = R(t-1) + (Recovered+Deceased) count from csv on day t\n",
    "- I(t) = I(t-1) + ( Infected count from csv on day t - Recovered count on day t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_susceptible_infected_recovered(data_frame):\n",
    "    \n",
    "    \n",
    "    infected_indices = [3*i for i in range(int(len(data_frame)/3))]\n",
    "    recovered_indices = [ 3*i+1 for i in range(int(len(data_frame)/3)) ]  # Add indices of Recovered\n",
    "    recovered_indices =  recovered_indices +  [ 3*i+2 for i in range(int(len(data_frame)/3)) ] # Add indices of Decesesed\n",
    "    recovered_indices = sorted(recovered_indices) \n",
    "\n",
    "    recovered_deceased = summed_data[recovered_indices]  # Get only rows related to recovered and deceased\n",
    "    recovered_dup = recovered_deceased.groupby(recovered_deceased.index//3).transform('sum')\n",
    "    rec_indices = [ 3*i + 1 for i in range(int(len(recovered_dup)//2))]\n",
    "    \n",
    "\n",
    "    recovered = recovered_dup[rec_indices]\n",
    "    recovered = recovered.reset_index(drop = True)\n",
    "    infected = summed_data[infected_indices]\n",
    "    infected = infected.reset_index(drop=True)\n",
    "\n",
    "    new_dataframe = pd.DataFrame({'infected' :  infected , 'recovered' : recovered})\n",
    "    time_series = new_dataframe.index.array\n",
    "\n",
    "\n",
    "    time_series_df = pd.DataFrame()\n",
    "    time_series_df.insert(0,\"time\",time_series)\n",
    "    time_series_df[\"S(t)\"] = 0\n",
    "    time_series_df[\"I(t)\"] = 0\n",
    "    \n",
    "    time_series_df.insert(3,\"R(t)\",new_dataframe['recovered'].cumsum()) # R(t) increases as more people recover each day So taking cumulative sum\n",
    "\n",
    "    time_series_df.loc[0,\"S(t)\"] = POPULATION- new_dataframe.loc[0,'infected'] # Infected people will only get deducted from susceptible\n",
    "    time_series_df.loc[0,\"I(t)\"] = new_dataframe.loc[0,'infected'] - new_dataframe.loc[0,'recovered'] # Formuala derived from I(t) = N - S(t) - R(t)\n",
    "    \n",
    "    # Continue for all timesteps\n",
    "    for i in range(1,len(new_dataframe)):\n",
    "    \n",
    "        time_series_df.loc[i,\"S(t)\"] = time_series_df.loc[i-1,\"S(t)\"] - new_dataframe.loc[i,'infected']\n",
    "        time_series_df.loc[i,\"I(t)\"] = time_series_df.loc[i-1,\"I(t)\"] + (new_dataframe.loc[i,'infected'] - new_dataframe.loc[i,'recovered'])\n",
    "    return time_series_df\n",
    "\n",
    "\n",
    "# To plot the time series graph for infected recovered and deceased.\n",
    "def plot_time_series(data_frame):\n",
    "\n",
    "    def set_axis_title(ax,title):\n",
    "        ax.set_ylabel('People Count')\n",
    "        ax.set_xlabel('Time in days')\n",
    "        ax.set_title(title)\n",
    "\n",
    "    figure,axes = plt.subplots(1,3)\n",
    "    figure.set_figwidth(15)\n",
    "\n",
    "    axes[0].plot(time_series_df['time'],time_series_df['S(t)'] ,color='b', label = 'S(t)')\n",
    "    axes[1].plot(time_series_df['time'],time_series_df['I(t)'] ,color='r', label = 'I(t)')\n",
    "    axes[2].plot(time_series_df['time'],time_series_df['R(t)'] ,color='g', label = 'R(t)')\n",
    "    set_axis_title(axes[0],'S(t)')\n",
    "    set_axis_title(axes[1],'I(t)')\n",
    "    set_axis_title(axes[2],'R(t)')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "time_series_df = get_susceptible_infected_recovered(summed_data)\n",
    "plot_time_series(time_series_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Least Square Problem Formulation\n",
    "\n",
    "- $\\beta$ and $\\gamma$ are the unknown parameters\n",
    "- $dS/dt$ , $dI/dt$ and $dR/dt$  at a particular time \"t\" can be estimated using the central difference method whose general form is\n",
    "   <br> <br> $\\frac{\\partial f(t)}{dt} = \\frac{f(t+1) - f(t-1)}{2}$\n",
    "\n",
    "- Edge cases handled by taking forward difference only and vice versa\n",
    "\n",
    "\n",
    "<br>\n",
    "We have the data samples of I(t), S(t) and R(t)  upto  time T. we can find \n",
    "\n",
    "$$\n",
    "\\left[\\begin{array}{c} \n",
    "\\beta \\\\\n",
    "\\gamma  \\\\\n",
    "\\end{array}\\right]\n",
    "$$ \n",
    "at time T as the solution to \n",
    "$$\n",
    "argmin_{\\beta,\\gamma}   \\textit{      } ||X \\left[\\begin{array}{c} \n",
    "\\beta \\\\\n",
    "\\gamma  \\\\\n",
    "\\end{array}\\right] -y ||_{2}^{2}\n",
    "$$\n",
    "where X and y are\n",
    "$$\n",
    "\\left[\\begin{array}{cc} \n",
    "-\\frac{I(1)*S(1)}{N} & 0\\\\ \\\\\n",
    "\\frac{I(1)*S(1)}{N} & -I(1) \\\\ \\\\\n",
    "0 & I(1) \\\\ \\\\\n",
    "-\\frac{I(2)*S(2)}{N} & 0\\\\ \\\\\n",
    "\\frac{I(2)*S(2)}{N} & -I(2) \\\\ \\\\\n",
    "0 & I(2) \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    ". \\\\\n",
    "-\\frac{I(T)*S(T)}{N} & 0\\\\ \\\\\n",
    "\\frac{I(T)*S(T)}{N} & -I(T) \\\\ \\\\\n",
    "0 & I(T) \\\\\n",
    "\\end{array}\\right]_{3*T \\text { x } 2}\n",
    "\\left[\\begin{array}{c} \n",
    "\\beta \\\\\n",
    "\\gamma\n",
    "\\end{array}\\right]\n",
    "= \n",
    "\\left[\\begin{array}{c} \n",
    "\\frac{dS(t)}{dt} |_{t=1}\\\\ \\\\\n",
    "\\frac{dI(t)}{dt} |_{t=1} \\\\ \\\\\n",
    "\\frac{dR(t)}{dt} |_{t=1} \\\\ \\\\\n",
    "\\frac{dS(t)}{dt} |_{t=2}\\\\ \\\\\n",
    "\\frac{dI(t)}{dt} |_{t=2} \\\\ \\\\\n",
    "\\frac{dR(t)}{dt} |_{t=2} \\\\ \\\\\n",
    ". \\\\ \\\\\n",
    ". \\\\ \\\\\n",
    "\\frac{dS(t)}{dt} |_{t=T}\\\\ \\\\\n",
    "\\frac{dI(t)}{dt} |_{t=T} \\\\ \\\\\n",
    "\\frac{dR(t)}{dt} |_{t=T} \\\\ \\\\\n",
    "\\end{array}\\right]\n",
    "$$ \n",
    "\n",
    "Converting to Normal Equations\n",
    "\n",
    "$$X^TX \\left[\\begin{array}{c} \n",
    "\\beta \\\\\n",
    "\\gamma  \\\\\n",
    "\\end{array}\\right] = X^Ty$$\n",
    "\n",
    "$$\n",
    "A \\left[\\begin{array}{c} \n",
    "\\beta \\\\\n",
    "\\gamma  \\\\\n",
    "\\end{array}\\right] = b\n",
    "$$\n",
    "where $A = X^TX$ and $b = X^Ty$\n",
    "\n",
    "Therefore we have formulated our problem as a Least Squares Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jacobian\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The vector valued function is \n",
    "    $$f(\\left[ \\begin{array}{c} \n",
    "    \\beta \\\\\n",
    "    \\gamma\n",
    "    \\end{array} \\right]) = \\left[\\begin{array}{c} \n",
    "\\frac{dS(t)}{dt} |_{t=1} \\\\ \\\\\n",
    "\\frac{dI(t)}{dt} |_{t=1} \\\\ \\\\\n",
    "\\frac{dR(t)}{dt} |_{t=1} \\\\ \\\\\n",
    "\\frac{dS(t)}{dt} |_{t=2} \\\\ \\\\\n",
    "\\frac{dI(t)}{dt} |_{t=2} \\\\ \\\\\n",
    "\\frac{dR(t)}{dt} |_{t=2} \\\\ \\\\\n",
    ". \\\\ \\\\\n",
    ". \\\\ \\\\\n",
    "\\frac{dS(t)}{dt} |_{t=T} \\\\ \\\\\n",
    "\\frac{dI(t)}{dt} |_{t=T} \\\\ \\\\\n",
    "\\frac{dR(t)}{dt} |_{t=T}\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c} \n",
    "\\frac {−βI(1)S(1)} {N} \\\\ \\\\\n",
    "\\frac {βI(1)S(1)} {N} −γI(1) \\\\ \\\\\n",
    "γI(1) \\\\ \\\\\n",
    "\\frac {−βI(2)S(2)} {N} \\\\ \\\\\n",
    "\\frac {βI(2)S(2)} {N} −γI(2) \\\\ \\\\\n",
    "γI(2) \\\\ \\\\\n",
    ". \\\\ \\\\\n",
    ". \\\\ \\\\\n",
    "\\frac {−βI(T)S(T)} {N} \\\\ \\\\\n",
    "\\frac {βI(T)S(T)} {N} −γI(T) \\\\ \\\\\n",
    "γI(T) \\\\ \\\\\n",
    "\\end{array}\\right] = \\left[\\begin{array}{c} \n",
    "f_{11}\\\\ \\\\\n",
    "f_{21} \\\\ \\\\\n",
    "f_{31} \\\\ \\\\\n",
    "f_{12}\\\\ \\\\\n",
    "f_{22} \\\\ \\\\\n",
    "f_{32} \\\\ \\\\\n",
    ". \\\\ \\\\ \n",
    ". \\\\ \\\\\n",
    "f_{1T}\\\\ \\\\\n",
    "f_{2T} \\\\ \\\\\n",
    "f_{3T} \\\\ \\\\\n",
    "\\end{array}\\right]$$  \n",
    "\n",
    "Calculating the Jacobian Matrix\n",
    "$$ J = \\left[\\begin{array}{cc} \n",
    "\\frac{df_{11}}{d\\beta} & \\frac{df_{11}}{d\\gamma} \\\\ \\\\\n",
    "\\frac{df_{21}}{d\\beta} & \\frac{df_{21}}{d\\gamma} \\\\ \\\\\n",
    "\\frac{df_{31}}{d\\beta} & \\frac{df_{31}}{d\\gamma} \\\\ \\\\\n",
    "\\frac{df_{12}}{d\\beta} & \\frac{df_{12}}{d\\gamma} \\\\ \\\\\n",
    "\\frac{df_{22}}{d\\beta} & \\frac{df_{22}}{d\\gamma} \\\\ \\\\\n",
    "\\frac{df_{32}}{d\\beta} & \\frac{df_{32}}{d\\gamma} \\\\ \\\\\n",
    ". \\\\ \\\\\n",
    ". \\\\ \\\\\n",
    "\n",
    "\\frac{df_{1T}}{d\\beta} & \\frac{df_{1T}}{d\\gamma} \\\\ \\\\\n",
    "\\frac{df_{2T}}{d\\beta} & \\frac{df_{2T}}{d\\gamma} \\\\ \\\\\n",
    "\\frac{df_{3T}}{d\\beta} & \\frac{df_{3T}}{d\\gamma}\n",
    "\\end{array}\\right] = \\left[\\begin{array}{cc} \n",
    "-\\frac{I(1)*S(1)}{N} & 0\\\\ \\\\\n",
    "\\frac{I(1)*S(1)}{N} & -I(1) \\\\ \\\\\n",
    "0 & I(1) \\\\ \\\\\n",
    "-\\frac{I(2)*S(2)}{N} & 0\\\\ \\\\\n",
    "\\frac{I(2)*S(2)}{N} & -I(2) \\\\ \\\\\n",
    "0 & I(2) \\\\ \\\\\n",
    ". \\\\ \\\\\n",
    ". \\\\ \\\\\n",
    "-\\frac{I(T)*S(T)}{N} & 0\\\\ \\\\\n",
    "\\frac{I(T)*S(T)}{N} & -I(T) \\\\ \\\\\n",
    "0 & I(T) \\\\ \\\\\n",
    "\\end{array}\\right]$$ \n",
    "\n",
    "The Jacobian matrix is equivalent to X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "# Jacobian Matrix Get Rank and Condition Number\n",
    "def get_rank_and_condition_number(data_frame):\n",
    "    J = np.array([[0.0 for i in range(2) ] for j in range(3*len(data_frame))])\n",
    "    for i in range(len(data_frame)):\n",
    "        current_day = data_frame.loc[i]\n",
    "        J[3*i,0] = -((current_day.loc['S(t)']*current_day.loc['I(t)'])/ POPULATION)\n",
    "        J[3*i+1,0] = ((current_day.loc['S(t)']*current_day.loc['I(t)'])/ POPULATION)\n",
    "        J[3*i+1,1] = -current_day.loc['I(t)']\n",
    "        J[3*i+2,1] = current_day.loc['I(t)']\n",
    "\n",
    "    rank = np.linalg.matrix_rank(J)\n",
    "    cn_number = np.linalg.cond(J)\n",
    "    return J,rank,cn_number\n",
    "\n",
    "jacobian, rank , cn_number = get_rank_and_condition_number(time_series_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Rank of Jacobian Matrix = \" + str(rank))\n",
    "print(\"Condition Number of Jacobian Matrix = \" + str(cn_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rank of Jacobian Matrix = 2 <br>\n",
    "Conditon Number of Jacobian Matrix $\\approx 1.73$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coefficient Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_number_coefficient_matrix = np.linalg.cond(np.matmul(jacobian.T,jacobian))\n",
    "print(\"Condition Number of Coefficient Matrix = \" + str(cond_number_coefficient_matrix))\n",
    "print(\"Rank of Coefficient Matrix = \" + str(np.linalg.matrix_rank(np.matmul(jacobian.T,jacobian))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Square of Condition number of Jacobian = \" + str(cn_number**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition number $\\kappa(A) = \\kappa(J^TJ) <= \\kappa(J^T)*\\kappa(J) = \\kappa(J)^2$  \n",
    "\n",
    "[Reference : https://scicomp.stackexchange.com/questions/23466/condition-number-of-rectangular-matrices]\n",
    "\n",
    "Therefore \n",
    "$$\n",
    "\\kappa(\\text{Coefficient Matrix}) <= (\\kappa(\\text{Jacobian}))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Solution to Least Squares Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Cholesky Factorization Alogrithm\n",
    "def cholesky(A):\n",
    "    n = A.shape[0]\n",
    "    ANS = np.zeros((A.shape[0],A.shape[1]))\n",
    "    for j in range(n):\n",
    "        s = 0.0\n",
    "        for k in range(j):\n",
    "            s += ANS[j][k] * ANS[j][k]\n",
    "        \n",
    "        ANS[j][j] = np.sqrt(A[j][j] - s)\n",
    "        for i in range(j+1,n):\n",
    "            s = 0.0\n",
    "            for k in range(0,j):\n",
    "                s += ANS[i][k] * ANS[j][k]\n",
    "            ANS[i][j] = (1.0/ANS[j][j] * (A[i][j]-s))\n",
    "    return ANS\n",
    "\n",
    "# Convert Matrix to Symmetric Matrix.\n",
    "def convert_to_symmetric(X):\n",
    "    return 0.5*(X + X.T)\n",
    "\n",
    "\n",
    "# Back Subsitution.\n",
    "def back_substitution(A,b):\n",
    "    solution = np.zeros(A.shape[0])\n",
    "    for i in range(A.shape[0]- 1, -1,-1):\n",
    "        tmp = b[i][0]\n",
    "\n",
    "        for j in range(A.shape[0]-1,i,-1):\n",
    "            tmp -= solution[j] * A[i,j]\n",
    "        solution[i] = tmp / A[i,i]\n",
    "    return solution\n",
    "\n",
    "# Forward Substitution Algorithm.\n",
    "def forward_substitute(A,b):\n",
    "    \n",
    "    solution = np.zeros(A.shape[0])\n",
    "    for i in range(A.shape[0]):        \n",
    "        tmp = b[i][0]\n",
    "        for j in range(i):\n",
    "            tmp -= solution[j]*A[i,j]\n",
    "        solution[i] = tmp / A[i,i]\n",
    "    return solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the linear system of equations \n",
    "# Returns the solution for cholesky factorization based computation and builtin function scipy.optimize.\n",
    "\n",
    "from scipy.optimize import nnls\n",
    "# Solve for beta and gamma for the data upto time t\n",
    "def solve(day_param,derivative):\n",
    "    '''\n",
    "        day_param : A dataframe with single row having columns S(t),I(t),R(t) for a single day\n",
    "        derivative : ds/dt, di/dt and dr/dt for a particular day.\n",
    "    '''\n",
    "    \n",
    "    num_days = len(day_param)\n",
    "    \n",
    "    \n",
    "    A_rank = set()  # Used to store the rank of matrix A (Existence of Cholesky Factorization)\n",
    "    X = np.zeros((3*num_days,2))\n",
    "    y = np.zeros((3*num_days,1))\n",
    "    \n",
    "    i_s_n = (day_param['S(t)'] * day_param['I(t)'])/ (POPULATION)\n",
    "\n",
    "    for i in range(len(i_s_n)):\n",
    "    \n",
    "        X[3*i,0] = -i_s_n.loc[i]\n",
    "        X[3*i+1,0] = i_s_n.loc[i]\n",
    "        X[3*i + 1,1] = -day_param.loc[i,'I(t)']\n",
    "        X[3*i + 2,1] = day_param.loc[i,'I(t)']  \n",
    "\n",
    "        y[3*i,0] = derivative.loc[i,'ds/dt']\n",
    "        y[3*i+1,0] = derivative.loc[i,'di/dt']\n",
    "        y[3*i+2,0] = derivative.loc[i,'dr/dt']\n",
    "\n",
    "    A = np.matmul(X.T,X)\n",
    "    b  = np.matmul(X.T,y)\n",
    "    A_rank.add(np.linalg.matrix_rank(A))\n",
    "    #A_sym = convert_to_symmetric(A)\n",
    "    \n",
    "    A_tri = cholesky(A)\n",
    "    inter = forward_substitute(A_tri,b)\n",
    "    inter = np.expand_dims(inter,1)\n",
    "\n",
    "    sol = back_substitution(A_tri.T,inter)\n",
    "    scipy_solution = nnls(X,y.squeeze())\n",
    "    return sol,scipy_solution,A_rank\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ds/dt, di/dt and dr/dt\n",
    "derivative_df = pd.DataFrame(0.0,index=np.arange(len(time_series_df)),columns=['ds/dt','di/dt','dr/dt'])\n",
    "\n",
    "for i in range(len(time_series_df)):\n",
    "    if i==0:\n",
    "        derivative_df.loc[i,\"ds/dt\"] = (time_series_df.loc[i+1,\"S(t)\"] - time_series_df.loc[i,\"S(t)\"])\n",
    "        derivative_df.loc[i,\"di/dt\"] = (time_series_df.loc[i+1,\"I(t)\"] - time_series_df.loc[i,\"I(t)\"])\n",
    "        derivative_df.loc[i,\"dr/dt\"] = (time_series_df.loc[i+1,\"R(t)\"] - time_series_df.loc[i,\"R(t)\"])\n",
    "    elif i == len(time_series_df)-1:\n",
    "        derivative_df.loc[i,\"ds/dt\"] = (time_series_df.loc[i,\"S(t)\"] - time_series_df.loc[i-1,\"S(t)\"])\n",
    "        derivative_df.loc[i,\"di/dt\"] = (time_series_df.loc[i,\"I(t)\"] - time_series_df.loc[i-1,\"I(t)\"])\n",
    "        derivative_df.loc[i,\"dr/dt\"] = (time_series_df.loc[i,\"R(t)\"] - time_series_df.loc[i-1,\"R(t)\"])\n",
    "    else:\n",
    "        derivative_df.loc[i,\"ds/dt\"] = (time_series_df.loc[i+1,\"S(t)\"] - time_series_df.loc[i-1,\"S(t)\"])/2.0\n",
    "        derivative_df.loc[i,\"di/dt\"] = (time_series_df.loc[i+1,\"I(t)\"] - time_series_df.loc[i-1,\"I(t)\"])/2.0\n",
    "        derivative_df.loc[i,\"dr/dt\"] = (time_series_df.loc[i+1,\"R(t)\"] - time_series_df.loc[i-1,\"R(t)\"])/2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = []\n",
    "plot_points = []\n",
    "scipy_solutions = []\n",
    "scipy_sol_error = []\n",
    "\n",
    "# Get Ro(t)\n",
    "for i in range(len(time_series_df)):\n",
    "    sol,scipy_solution,MatrixA_rank = solve(time_series_df[:i+1],derivative_df[:i+1])\n",
    "    scipy_solutions.append(scipy_solution[0][0]/scipy_solution[0][1])\n",
    "    scipy_sol_error.append(scipy_solution[1])\n",
    "    plot_points.append(sol[0]/sol[1])\n",
    "    indices.append(i+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.ylabel('Ro(t)')\n",
    "plt.plot(indices,plot_points)\n",
    "plt.show()\n",
    "plt.ylabel('Ro(t)')\n",
    "plt.plot(indices,scipy_solutions,'--',color = 'orange',alpha=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overlaying the plots on one another\n",
    "plt.ylabel('Ro(t)')\n",
    "plt.plot(indices,plot_points)\n",
    "plt.plot(indices,scipy_solutions,'--',color = 'orange',alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Couldnt find any visible difference between the plots Ro(t) from the values computed using cholesky and scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if Ro(t) computed from cholesky and scipy are the same\n",
    "# Computing the absolute difference between the two summing and averaging up\n",
    "error = 0.0\n",
    "for i in range(len(indices)):\n",
    "    error += np.abs(scipy_solutions[i] - plot_points[i] )\n",
    "    #print(str(scipy_solutions[i]) + \"    ;;;; \" + str(plot_points[i]) + \",,,,\" + str(scipy_sol_error[i])) \n",
    "print(\"Average error = \" + str(error/len(indices)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average error is very small (10^-15). The solutions obtained using the cholesky factorization based solver and builtin solver is almost equal to each other.\n",
    "\n",
    "- Since the rank of the Coefficient Matrix is 2 which is equal to number of variables, a solution to the the linear system of equations which we formulated exist.\n",
    "\n",
    "- Since the condition number of the Coefficient Matrix is not strictly larger than 1,The matrix is well conditioned which means inverse can be computed with good accuracy [Reference : https://en.wikipedia.org/wiki/Condition_number]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming into the 'Ro(t) vs t' plot from t=100 onwards "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zooming in towards the 100 th day\n",
    "\n",
    "plt.ylabel('Ro(t)')\n",
    "plt.plot(indices[100:],plot_points[100:])\n",
    "plt.show()\n",
    "\n",
    "# Zooming in towards the 200 th day\n",
    "plt.ylabel('Ro(t)')\n",
    "plt.plot(indices[200:],plot_points[200:])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Towards the end we can see  declining Ro(t) values.\n",
    "\n",
    "- Upon manual observation of values of Ro(t) towards the end (>100th day and > 200 th day ) We can see that Ro values were above 1 for <180 th day and the the Ro(t) values closer to 1 towards the end of October.\n",
    "\n",
    "-  We cant clearly tell whether a peak is present ($\\approx 1.07$) but seeing the declining trend and the closeness of values towards 1.0, we can say that the peak has passed as of Oct 2020 and the pandemic is under control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Question : 2** \n",
    "\n",
    "\n",
    "**Steepest Descent and Newton's Line Search Methods**\n",
    "\n",
    "\n",
    "1. Find the minima $x^*$ for the given functions $f_1(x)$ and  $f_2(x)$ using your own implementation of Steepest Descent. Compute the step lenght by implementing the backtracking algorithm (Algorithm 3.1 Nocedal and Wright) with $\\rho = 0.9$ and $c = 10^{-4}$. **[1.5 Points]**\n",
    "\n",
    "2. Find the minima $x^*$ for the given functions $f_1(x)$ and  $f_2(x)$ using your own implementation of Newton's Method. **[1 Point]**\n",
    "\n",
    "Notes:\n",
    "1. Run both algorithms for two initial guesses. i. $x_0=(2,0)$ and ii. $x_0=(2,2)$\n",
    "2. Stop iterations when $||x_{k+1} - x_{k}||_2^2 < 10^{-5}$\n",
    "3. For each case report the solution and the number of iterations to converge. Also comment on the reported number of iterations.\n",
    "4. Show the function contour plot and the iterates {$x_k$} including the solution.\n",
    "\n",
    "Consider the following quadratic functions:\n",
    "1. $f_1(x) = \\frac{1}{2} x^T A_1 x $\n",
    "  \n",
    "  where \n",
    "$A_1 = \n",
    "  \\begin{pmatrix}\n",
    "  1 & 0 \\\\\n",
    "  0 & 1 \\\\\n",
    "  \\end{pmatrix}$\n",
    "\n",
    "2. $f_2(x) = \\frac{1}{2} x^T A_2 x $\n",
    "  \n",
    "  where \n",
    "$A_2 = \n",
    "  \\begin{pmatrix}\n",
    "  10 & 8 \\\\\n",
    "  8 & 10 \\\\\n",
    "  \\end{pmatrix}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some Helper functions \n",
    "def squared_norm(x):\n",
    "    x_val = np.float64(x[0][0])\n",
    "    y_val = np.float64(x[1][0])\n",
    "    return x_val*x_val + y_val*y_val\n",
    "\n",
    "def norm(x):\n",
    "    return np.sqrt(squared_norm(x))\n",
    "\n",
    "def get_unit_vector(x):\n",
    "    norm_value = norm(x)\n",
    "    if norm_value == np.float64(0.0):\n",
    "        return x\n",
    "    return (x/norm_value).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the functions,\n",
    "# The Jacobians  and the hessian\n",
    "\n",
    "def f1(x,y):\n",
    "\n",
    "    # x,y  = Elements of 2*1 vector\n",
    "    vec = np.array([[x],[y]])\n",
    "    A = np.array([[1,0],[0,1]])\n",
    "    h = np.matmul(vec.T,A)\n",
    "    return 0.5*np.matmul(h,vec)\n",
    "\n",
    "def f2(x,y):\n",
    "    \n",
    "    # x,y  = Elements of 2*1 vector\n",
    "    vec = np.array([[x],[y]])\n",
    "    A = np.array([[10,8],[8,10]])\n",
    "    h = np.matmul(vec.T,A)\n",
    "    return 0.5*np.matmul(h,vec)\n",
    "\n",
    "\n",
    "def gradient_f1(x):\n",
    "    \n",
    "    # x = Input 2*1 vector\n",
    "    x_value = np.float64(x[0][0])\n",
    "    y_value = np.float64(x[1][0])\n",
    "    return np.array([[x_value],[y_value]])\n",
    "\n",
    "def gradient_f2(x):\n",
    "    # x =  2*1 vector\n",
    "    x_value = np.float64(x[0][0])\n",
    "    y_value = np.float64(x[1][0])\n",
    "    return np.array([[10*x_value + 8 *y_value],[10*y_value + 8 * x_value]])\n",
    "\n",
    "def hessian_f1(x):\n",
    "    # x =  2*1 vector\n",
    "    return np.array([[1,0],[0,1]])\n",
    "\n",
    "def hessian_f2(x):\n",
    "    # x =  2*1 vector\n",
    "    return np.array([[10,8],[8,10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent(initial_point,function='f1'):\n",
    "    '''\n",
    "        Arguments\n",
    "            - initial_point = 2*1 vector specifying the initial point.\n",
    "            - function = String to refer to the function which needs to be optimized.\n",
    "        \n",
    "    '''\n",
    "    fun = f1 if function == 'f1' else f2\n",
    "    gradient_fun = gradient_f1 if function == 'f1' else gradient_f2\n",
    "\n",
    "    current_point = initial_point.copy()\n",
    "    c = 0.0001\n",
    "    rho = 0.9\n",
    "    iterations = 0\n",
    "    iterates = []\n",
    "\n",
    "    iterates.append(current_point)\n",
    "    while True:        \n",
    "        \n",
    "        iterations += 1\n",
    "        gradient = gradient_fun(current_point)\n",
    "        # Direction of p should be opposite to the direction of the gradient\n",
    "        p = -get_unit_vector(gradient)       \n",
    "\n",
    "        # Intial step size     \n",
    "        alpha  = np.float64(1.0)\n",
    "        previous_point = current_point.copy() \n",
    "        \n",
    "        \n",
    "        break_inner_loop = False\n",
    "        current_x = np.float64(current_point[0][0])\n",
    "        current_y = np.float64(current_point[1][0])\n",
    "        current_function_value = np.float64(fun(current_x,current_y).squeeze()) \n",
    "        \n",
    "        \n",
    "        while not break_inner_loop:\n",
    "            \n",
    "            new_point = current_point + alpha*p\n",
    "\n",
    "            new_x = np.float64(new_point[0][0])\n",
    "            new_y = np.float64(new_point[1][0])\n",
    "\n",
    "            # Calculate the terms in the Taylor Series Approximation.\n",
    "            new_function_value = np.float64(fun(new_x,new_y).squeeze())\n",
    "            increment_value = np.float64(((c*alpha)*np.dot(gradient.T,p)).squeeze())\n",
    "\n",
    "            # Decreasing the step size until sufficient decrease\n",
    "            if((new_function_value > (current_function_value + increment_value))):\n",
    "                alpha = alpha * rho\n",
    "            else:\n",
    "                break_inner_loop = True\n",
    "     \n",
    "\n",
    "        # Perform descent using the step size computed\n",
    "        new_point = current_point + alpha*p\n",
    "        current_point = new_point.copy()\n",
    "        iterates.append(current_point)\n",
    "        # Condition for stopping \n",
    "        if np.linalg.norm(current_point-previous_point) < 0.00001:\n",
    "            break\n",
    "    return iterations,iterates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newtons_method(initial_point,function = 'f1'):\n",
    "    \n",
    "    '''\n",
    "        Arguments\n",
    "            - initial_point = 2*1 vector specifying the initial point.\n",
    "            - function = String to refer to the function which needs to be optimized.\n",
    "        \n",
    "    '''\n",
    "\n",
    "    fun = f1 if function == 'f1' else f2\n",
    "    gradient_fun = gradient_f1 if function == 'f1' else gradient_f2\n",
    "    hessian_fun = hessian_f1 if function =='f1' else hessian_f2   # Required for calculating the step's direction\n",
    "    \n",
    "\n",
    "    current_point = initial_point.copy()\n",
    "    iterations = 0\n",
    "    iterates = []\n",
    "    iterates.append(current_point)\n",
    "\n",
    "\n",
    "    # For performing the line search \n",
    "    c = 0.0001\n",
    "    rho = 0.9\n",
    "\n",
    "    while True:        \n",
    "        iterations+=1\n",
    "        gradient = gradient_fun(current_point)\n",
    "        hessian = hessian_fun(current_point)\n",
    "\n",
    "        # Step Direction in Newtons Method\n",
    "        m = np.matmul(np.linalg.inv(hessian),gradient)\n",
    "        p = -m\n",
    "        \n",
    "        break_inner_loop = False\n",
    "        \n",
    "        # Initial Step Size\n",
    "        alpha  = np.float64(1.0)\n",
    " \n",
    "        current_x = np.float64(current_point[0][0])\n",
    "        current_y = np.float64(current_point[1][0])\n",
    "        current_function_value = np.float64(fun(current_x,current_y).squeeze()) \n",
    "        \n",
    "        while not break_inner_loop:\n",
    "            new_point = current_point + alpha*p\n",
    "            new_x = np.float64(new_point[0][0])\n",
    "            new_y = np.float64(new_point[1][0])\n",
    "           \n",
    "           \n",
    "            # Calculating the taylor Series Approximation\n",
    "            new_function_value = np.float64(fun(new_x,new_y).squeeze())\n",
    "            increment_value = np.float64(((c*alpha)*np.dot(gradient.T,p)).squeeze())\n",
    "            second_order_term = (c*(alpha*alpha/2)*(np.matmul(np.matmul(p.T,hessian),p))).squeeze()\n",
    "            increment_value += np.float64(second_order_term)\n",
    "\n",
    "\n",
    "            # Determining the  appropirate stp size\n",
    "            if((new_function_value > (current_function_value + increment_value))):\n",
    "                alpha = alpha * rho\n",
    "            else:\n",
    "                break_inner_loop = True\n",
    "      \n",
    "        \n",
    "        previous_point = current_point.copy() \n",
    "        # Perform the descent\n",
    "        new_point = current_point + alpha*p\n",
    "        current_point = new_point.copy()\n",
    "        iterates.append(current_point)\n",
    "        if np.linalg.norm(current_point-previous_point) < 0.00001:\n",
    "            break\n",
    "    \n",
    "    return iterations,iterates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Contour Plots\n",
    "def plot_contour(function,iterations,iterates,title = 'Contour Plot'):\n",
    "\n",
    "    if function == 'f1':\n",
    "        fun = f1\n",
    "    else:\n",
    "        fun = f2\n",
    "\n",
    "    res = 1000\n",
    "    x_values = np.linspace(2.1,-2.1,res)\n",
    "    y_values = np.linspace(2.1,-2.1,res)\n",
    "    X,Y = np.meshgrid(x_values,y_values)\n",
    "\n",
    "    Z = np.zeros((res,res))\n",
    "    for i in range(len(X[0])):\n",
    "        for j in range(len(Y[:,0])):\n",
    "            z_value = fun(X[0,i],Y[j,0])\n",
    "            Z[i][j] = z_value\n",
    "    \n",
    "    fig,ax=plt.subplots(1,1)\n",
    "    cp = ax.contourf(X, Y, Z)\n",
    "    fig.colorbar(cp) \n",
    "    ax.set_title(title)\n",
    "    plt.plot(iterates[0][0,0],iterates[0][1,0],'ro')\n",
    "    for i in range(1,len(iterates)):\n",
    "        plt.arrow(iterates[i-1][0,0],iterates[i-1][1,0],dx = iterates[i][0,0] - iterates[i-1][0,0] ,dy = iterates[i][1,0] - iterates[i-1][1,0],width=0.01,head_width = 0.09)\n",
    "    plt.plot(iterates[len(iterates)-1][0,0], iterates[len(iterates)-1][1,0],'go',linewidth=0.1)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function f1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red Dot Stands for the starting Point\n",
    "# Green Dot Stands for the ending point\n",
    "\n",
    "\n",
    "initial_point = np.array([[2],[0]])\n",
    "\n",
    "print(\"Initial Point = [[2],[0]]\")\n",
    "print(\"----------------------------\")\n",
    "iterations_steep_f1 , iterates_steep_f1 = steepest_descent(initial_point,'f1')\n",
    "iterations_newton_f1 , iterates_newton_f1 = newtons_method(initial_point,'f1')\n",
    "print(\"Steepest Descent Solution = \" + str(iterates_steep_f1[-1]))\n",
    "print(\"Iterations taken for Steepest Descent= \" + str(iterations_steep_f1))\n",
    "plot_contour('f1',iterations_steep_f1,iterates_steep_f1,title='Steepest Descent ; Function = f1 ; Initial Point = [[2][0]]' )\n",
    "print(\"Newtons Method Solution = \" + str(iterates_newton_f1[-1]))\n",
    "print(\"Iterations taken for Newtons Method = \" + str(iterations_newton_f1))\n",
    "plot_contour('f1',iterations_newton_f1,iterates_newton_f1,title='Newtons Method ; Function = f1 ; Initial Point = [[2][0]]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_point = np.array([[2],[2]])\n",
    "\n",
    "print(\"Initial Point = [[2],[2]]\")\n",
    "print(\"----------------------------\")\n",
    "iterations_steep_f1 , iterates_steep_f1 = steepest_descent(initial_point,'f1')\n",
    "iterations_newton_f1 , iterates_newton_f1 = newtons_method(initial_point,'f1')\n",
    "print(\"Steepest Descent Solution = \" + str(iterates_steep_f1[-1]))\n",
    "print(\"Iterations taken for Steepest Descent= \" + str(iterations_steep_f1))\n",
    "plot_contour('f1',iterations_steep_f1,iterates_steep_f1,title='Steepest Descent ; Function = f1 ; Initial Point = [[2][2]]' )\n",
    "print(\"Newtons Method Solution = \" + str(iterates_newton_f1[-1]))\n",
    "print(\"Iterations taken for Newtons Method =\" + str(iterations_newton_f1))\n",
    "plot_contour('f1',iterations_newton_f1,iterates_newton_f1,title='Newtons Method ; Function = f1 ; Initial Point = [[2][2]]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Function f2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_point = np.array([[2],[0]])\n",
    "\n",
    "print(\"Initial Point = [[2],[0]]\")\n",
    "print(\"----------------------------\")\n",
    "iterations_steep_f2 , iterates_steep_f2 = steepest_descent(initial_point,'f2')\n",
    "iterations_newton_f2 , iterates_newton_f2 = newtons_method(initial_point,'f2')\n",
    "print(\"Steepest Descent Solution = \" + str(iterates_steep_f2[-1]))\n",
    "print(\"Iterations taken for Steepest Descent= \" + str(iterations_steep_f2))\n",
    "plot_contour('f2',iterations_steep_f2,iterates_steep_f2,title='Steepest Descent ; Function = f2 ; Initial Point = [[2][0]]' )\n",
    "print(\"Newtons Method Solution = \" + str(iterates_newton_f2[-1]))\n",
    "print(\"Iterations taken for Newtons Method =\" + str(iterations_newton_f2))\n",
    "plot_contour('f2',iterations_newton_f2,iterates_newton_f2,title='Newtons Method ; Function = f2 ; Initial Point = [[2][0]]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_point = np.array([[2],[2]])\n",
    "\n",
    "print(\"Initial Point = [[2],[2]]\")\n",
    "print(\"----------------------------\")\n",
    "iterations_steep_f2 , iterates_steep_f2 = steepest_descent(initial_point,'f2')\n",
    "iterations_newton_f2 , iterates_newton_f2 = newtons_method(initial_point,'f2')\n",
    "print(\"Steepest Descent Solution = \" + str(iterates_steep_f2[-1]))\n",
    "print(\"Iterations taken for Steepest Descent= \" + str(iterations_steep_f2))\n",
    "plot_contour('f2',iterations_steep_f2,iterates_steep_f2,title='Steepest Descent ; Function = f1 ; Initial Point = [[2][2]]' )\n",
    "print(\"Newtons Method Solution = \" + str(iterates_newton_f2[-1]))\n",
    "print(\"Iterations taken for Newtons Method =\" + str(iterations_newton_f2))\n",
    "plot_contour('f2',iterations_newton_f2,iterates_newton_f2,title='Newtons Method ; Function = f1 ; Initial Point = [[2][2]]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "f_1(x) = \\frac{1}{2} x^T A_1 x \n",
    "  \n",
    "$$\n",
    "$$\n",
    "A_1 = \n",
    "  \\begin{pmatrix}\n",
    "  1 & 0 \\\\\n",
    "  0 & 1 \\\\\n",
    "  \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "| Initial Point     | Iterations(Steepest Descent) | Minima(Steepest Descent) | Iterations (Newtons Method) | Minima (Newtons Method) | \n",
    "| ----------- | ----------- |-----| ----- | ----- | \n",
    "| (2,2)      | 72      |   $(2*10^{-6},2*10^{-6})$  | 2 | (0.0,0.0) | \n",
    "| (2,0)  | 3       |  (0.0,0.0)  | 2 | (0.0,0.0) | \n",
    "\n",
    "\n",
    "\n",
    "$$f_2(x) = \\frac{1}{2} x^T A_2 x \n",
    "  \n",
    "$$\n",
    "$$ A_2 = \n",
    "  \\begin{pmatrix}\n",
    "  10 & 8 \\\\\n",
    "  8 & 10 \\\\\n",
    "  \\end{pmatrix} $$\n",
    "\n",
    "\n",
    "| Initial Point     | Iterations(Steepest Descent) | Minima(Steepest Descent) | Iterations (Newtons Method) | Minima (Newtons Method) | \n",
    "| ----------- | ----------- |-----| ----- | ----- | \n",
    "| (2,2)      | 72      |   $(2*10^{-6},2*10^{-6})$  | 2 | (0.0,0.0) | \n",
    "| (2,0)  | 90       | $(-2*10^{-6},-2*10^{-6})$   | 2 | ($9*10^{-32}$,0.0) | \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations\n",
    "\n",
    "- Newtons Method took only two iterations to converge to a solution (One Iteration to move and one iteration to check condition) in all the cases\n",
    "$$\n",
    "    f = \\frac{1}{2} x^TAx  \\\\ \\\\\n",
    "$$\n",
    "    \n",
    "$$\n",
    "    \\nabla f = Ax  \\\\ \\\\  \\\\\n",
    "\n",
    "  $$\n",
    "  $$  \n",
    "    \\nabla^2 f = A\n",
    "$$\n",
    "  \n",
    "The Newtons method update the parameters as $x_1 = x_0 -\\alpha(\\nabla^2 f)^{-1} \\nabla f$. <br>\n",
    "  When $alpha = 1$ and substituting for $\\nabla^2 f$ and $\\nabla f$ <br>\n",
    "  \n",
    "\n",
    "$$\n",
    "  x_1 = x_0 - A^{-1}(Ax_0)\n",
    "$$\n",
    "Since both A's given are symmetric positive definite inverse of A exist. So\n",
    "$$\n",
    "  x_1 = x_0 - x_0 = \\vec{0}\n",
    "$$\n",
    "\n",
    "We have started with initial step size as 1 and indeed it provides a decrease in function value on substitution . Therefore we can see that Newtons method is able to converge in two iterations.\n",
    "\n",
    "\n",
    "<br><br>\n",
    "- The steepest descent method takes the most amount of time compared to Newtons Method\n",
    "\n",
    "- As we reach closer and closer to the minima the step sizes to take becomes smaller and smaller giving rise to more iterations to converge.\n",
    "\n",
    "- When the initial point is along an eigen vector of A, We can see that the steepest method follows an almost Straight path to the minima (Inital point (2,2) in both functions $f1$ and $f2$ and Initial point (2,0) in function $f1$ ). We could see the steepest descent following a zig zag path for the initial point (2,0) in $f2$ (Blindly following descent direction).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Question : 3**\n",
    "\n",
    "**Rosenbrock function**\n",
    "\n",
    "Use the steepest descent and Newtons algorithms using the backtracking line search to minimize the classic Rosenbrock function. Set the initial step length to 1. At each iteration store the step lengths used by each method and make plots. Show the step lengths taken and iterates as plots. Do these for a start point of search $x_0 = [1.2, 1.2]^T$ and then for the starting point $x_0 = [-1.2, 1]^T$ **[1 Point]**\n",
    "\n",
    "1. Plot the convergence of the iterates and the objective function value. Evaluate the rate of convergence. **[0.5 Points]**\n",
    "\n",
    "2. Call built-in functions for steepest descent and newton’s method, and show the results for the above. Compare and evaluate your program. Compare the run-time of your program and built-in function. Is there a difference? Why or why not? **Hint**: Jacobians! **[1 Point]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import rosen,rosen_der\n",
    "from scipy.optimize import line_search\n",
    "from scipy.optimize import minimize\n",
    "from scipy.optimize import approx_fprime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the functions needed.\n",
    "def rosenbrock(point):\n",
    "    a = 1.0\n",
    "    b = 100.0\n",
    "    x = point[0]\n",
    "    y = point[1]\n",
    "    result = (1-x)**2\n",
    "    result += b*((y - x**2)**2)\n",
    "    return result\n",
    "\n",
    "# Jacobian of the RosenBrock Function\n",
    "def rosenbrock_jacobian(point):\n",
    "    a = 1.0\n",
    "    b = 100.0\n",
    "    x = point[0]\n",
    "    y = point[1]\n",
    "    result = np.zeros((2,))\n",
    "    result[0] = -2.0*(a-x) - 4*b*x*(y - x**2)\n",
    "    result[1] = 2.0*b*(y - x**2)\n",
    "    return result\n",
    "\n",
    "# Hessian of the RosenBrock Function\n",
    "def rosenbrock_hessian(point):\n",
    "    a = 1.0\n",
    "    b = 100.0\n",
    "    x = point[0]\n",
    "    y = point[1]\n",
    "    hessian = np.zeros((2,2))\n",
    "    hessian[0][0] = 2+12*b*x*x - 4*b*y\n",
    "    hessian[0][1] = -4*b*x\n",
    "    hessian[1][0] = -4*b*x\n",
    "    hessian[1][1] = 2*b \n",
    "    return hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent_rosenbrock(point, no_stopping_criterion=False, return_extra_info = True):\n",
    "    # No stopping criterion is used for collecting enough number of samples to evaluate the rate of converge\n",
    "    # We are doing steepest descent on the RosenBrock Function\n",
    "    fun = rosenbrock\n",
    "    gradient_fun = rosenbrock_jacobian\n",
    "    \n",
    "    # Get a copy of the initial point\n",
    "    current_point = point.copy()\n",
    "    \n",
    "    # Parameters for the steepest descent\n",
    "    c = 0.0001\n",
    "    rho = 0.9\n",
    "\n",
    "    # The parameters to be returned\n",
    "    iterations = 0\n",
    "    \n",
    "    if return_extra_info:\n",
    "        step_sizes = []\n",
    "        iterates = []\n",
    "        function_values = []\n",
    "    \n",
    "        iterates.append(current_point)\n",
    "\n",
    "        function_values.append(np.float64(fun(current_point.squeeze()).squeeze()))\n",
    "   \n",
    "    while True:        \n",
    "        iterations += 1\n",
    "        \n",
    "        # Calculate the descent direction.\n",
    "        gradient = gradient_fun(current_point)\n",
    "        gradient = np.expand_dims(gradient,axis=1)\n",
    "        p = -get_unit_vector(gradient)            \n",
    "        \n",
    "        # Initial Step Size\n",
    "        alpha  = np.float64(1.0)\n",
    "        previous_point = current_point.copy() \n",
    "        break_inner_loop = False\n",
    "    \n",
    "        # Function value at the current point.\n",
    "        current_function_value = np.float64(fun(current_point.squeeze()).squeeze()) \n",
    "        \n",
    "        # BackTracking Alogrithm.\n",
    "        while not break_inner_loop:\n",
    "            \n",
    "            new_point = current_point + alpha*p\n",
    "\n",
    "            new_function_value = np.float64(fun(new_point.squeeze()).squeeze())\n",
    "            increment_value = np.float64(((c*alpha)*np.dot(gradient.T,p)).squeeze())\n",
    "\n",
    "            if((new_function_value > (current_function_value + increment_value))):\n",
    "                alpha = alpha * rho\n",
    "            else:\n",
    "                break_inner_loop = True\n",
    "     \n",
    "        # Perform descent\n",
    "        new_point = current_point + alpha*p\n",
    "        current_point = new_point.copy()\n",
    "\n",
    "        if return_extra_info:\n",
    "            step_sizes.append(alpha)\n",
    "           \n",
    "            iterates.append(current_point)\n",
    "            function_values.append(np.float64(fun(new_point.squeeze()).squeeze()))\n",
    "\n",
    "        # Stopping Criterion\n",
    "        if not no_stopping_criterion:\n",
    "            if np.linalg.norm(current_point-previous_point) < 0.00001:\n",
    "                break\n",
    "        else:\n",
    "            if(iterations >= 3000):\n",
    "              break\n",
    "    if return_extra_info:\n",
    "        return iterations,iterates, function_values, step_sizes\n",
    "    else:\n",
    "        return iterations,current_point\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_rosenbrock(point, no_stopping_criterion = False,return_extra_info = True):\n",
    "    # No stopping criterion is used for collecting enough number of samples to evaluate the rate of converge\n",
    "    # We are doing Newtons Method on the RosenBrock Function\n",
    "    fun = rosenbrock\n",
    "    gradient_fun = rosenbrock_jacobian\n",
    "    hessian_fun = rosenbrock_hessian\n",
    "    \n",
    "    \n",
    "    current_point = point.copy()\n",
    "    \n",
    "    # The items which needs to be returned.\n",
    "    iterations = 0\n",
    "    \n",
    "    if return_extra_info:\n",
    "        function_values = []\n",
    "        iterates = []\n",
    "        step_sizes = []\n",
    "\n",
    "        iterates.append(current_point)\n",
    "        function_values.append(np.float64(fun(current_point.squeeze()).squeeze()))\n",
    "\n",
    "    # Parameters for determining the step length. \n",
    "    c = 0.0001\n",
    "    rho = 0.9\n",
    "\n",
    "    while True:        \n",
    "        \n",
    "        iterations+=1\n",
    "        gradient = np.expand_dims(gradient_fun(current_point.squeeze()),axis=1)\n",
    "        hessian = hessian_fun(current_point.squeeze())\n",
    "        \n",
    "        # The Step Direction\n",
    "        m = np.matmul(np.linalg.inv(hessian),gradient)\n",
    "        p = -m\n",
    "        \n",
    "        break_inner_loop = False\n",
    "        alpha  = np.float64(1.0)\n",
    "        current_function_value = np.float64(fun(current_point.squeeze()).squeeze()) \n",
    "       \n",
    "       # Backtracking search to determine the step length.\n",
    "        while not break_inner_loop:\n",
    "\n",
    "            new_point = current_point + alpha*p\n",
    "            new_function_value = np.float64(fun(new_point.squeeze()).squeeze())\n",
    "            increment_value = np.float64(((c*alpha)*np.dot(gradient.T,p)).squeeze())\n",
    "            second_order_term = (c*(alpha*alpha/2)*(np.matmul(np.matmul(p.T,hessian),p))).squeeze()\n",
    "            increment_value += np.float64(second_order_term)\n",
    "            \n",
    "\n",
    "            if((new_function_value > (current_function_value + increment_value))):\n",
    "                alpha = alpha * rho\n",
    "            else:\n",
    "                break_inner_loop = True\n",
    "\n",
    "        \n",
    "        previous_point = current_point.copy() \n",
    "        new_point = current_point + alpha*p\n",
    "        current_point = new_point.copy()\n",
    "        \n",
    "        if return_extra_info:\n",
    "            step_sizes.append(alpha)\n",
    "            function_values.append(np.float64(fun(new_point.squeeze()).squeeze()))\n",
    "            iterates.append(current_point)\n",
    "        \n",
    "\n",
    "        # Stopping Condition\n",
    "        if not no_stopping_criterion:\n",
    "            if np.linalg.norm(current_point-previous_point) < 0.00001:\n",
    "                break\n",
    "        else:\n",
    "            if (iterations >= 3000):\n",
    "                break\n",
    "    \n",
    "    if return_extra_info:\n",
    "        return iterations,iterates,function_values,step_sizes\n",
    "    else:\n",
    "        return iterations,current_point\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper Function for plotting the step length as a function of time and iterates \n",
    "def plot_steplength_iterates(step_sizes,iterates,lower_limit,upper_limit,title1='Contour plot',title2 = 'Step Size'):\n",
    "  \n",
    "\n",
    "    res = 3000\n",
    "    x_values = np.linspace(lower_limit,upper_limit,res)\n",
    "    y_values = np.linspace(lower_limit,upper_limit,res)\n",
    "    X,Y = np.meshgrid(x_values,y_values)\n",
    "\n",
    "    plot_fn = lambda x,y : rosenbrock(np.array([x,y]))\n",
    "    Z = plot_fn(X,Y)    \n",
    "\n",
    "\n",
    "    fig,ax=plt.subplots(1,2)\n",
    "    fig.set_figwidth(15)\n",
    "    cp = ax[0].contour(X, Y, Z,500)\n",
    "    fig.colorbar(cp) \n",
    "    ax[0].set_title(title1)\n",
    "    \n",
    "    ax[0].plot(iterates[0,0,0],iterates[0,1,0],'ro')\n",
    "    ax[0].plot(iterates[:,0,0],iterates[:,1,0])\n",
    "    ax[0].plot(iterates[len(iterates)-1,0,0], iterates[len(iterates)-1,1,0],'go')\n",
    "    \n",
    "    ax[1].plot(np.arange(1,len(iterates))[:150],step_sizes[:150])\n",
    "    ax[1].set_xlabel('Epoch')\n",
    "    ax[1].set_ylabel(title2)  \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "##### Initial Point (1.2,1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_point = np.array([[1.2],[1.2]])\n",
    "iterations_steep_p1, iterates_steep_p1, function_values_steep_p1, step_sizes_steep_p1 = steepest_descent_rosenbrock(initial_point)\n",
    "iterations_newton_p1, iterates_newton_p1, function_values_newton_p1, step_sizes_newton_p1 = newton_rosenbrock(initial_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Red Dot Stands for the starting Point\n",
    "# Green Dot Stands for the ending point\n",
    "\n",
    "print (\"Steepest Descent Iterates and Step sizes\")\n",
    "plot_steplength_iterates(step_sizes_steep_p1,np.array(iterates_steep_p1),-2.0,2.0,title1='Contour Plot; Initial Point = [1.2,1.2]')\n",
    "print (\"Newtons Method Iterates and Step sizes\")\n",
    "plot_steplength_iterates(step_sizes_newton_p1,np.array(iterates_newton_p1),-2.0,2.0,title1='Contour Plot; Initial Point = [1.2,1.2]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initial Point = [-1.2,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_point = np.array([[-1.2],[1.0]])\n",
    "iterations_steep_p2, iterates_steep_p2, function_values_steep_p2, step_sizes_steep_p2 = steepest_descent_rosenbrock(initial_point)\n",
    "iterations_newton_p2, iterates_newton_p2, function_values_newton_p2, step_sizes_newton_p2 = newton_rosenbrock(initial_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Steepest Descent Iterates and Step sizes\")\n",
    "plot_steplength_iterates(step_sizes_steep_p2,np.array(iterates_steep_p2),-2.0,2.0,title1='Contour Plot; Initial Point = [-1.2,1.0]')\n",
    "print (\"Newtons Method Iterates and Step sizes\")\n",
    "plot_steplength_iterates(step_sizes_newton_p2,np.array(iterates_newton_p2),-2.0,2.0,title1='Contour Plot; Initial Point = [-1.2,1.0]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rate Of Convergence \n",
    "\n",
    "Allowing the algorithms to run upto a maximum of 3000 iterations without any stopping criterion in between.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def convergence(iterates,minima,q):\n",
    "    \n",
    "    convergence_values = []\n",
    "\n",
    "    for i in range(1,len(iterates)):\n",
    "        n1 = np.linalg.norm(iterates[i]-minima)\n",
    "        n2 = np.linalg.norm(iterates[i-1]-minima)\n",
    "\n",
    "        conv_value = n1 / np.power(n2,q)\n",
    "        convergence_values.append(conv_value)\n",
    "    return convergence_values\n",
    "\n",
    "    \n",
    "def plot_convergence_and_fnvalues(function_values,convergences,title):\n",
    "\n",
    "    fig,ax = plt.subplots(1,2)\n",
    "\n",
    "        \n",
    "\n",
    "    c_indices = [j for j in range(len(convergences[0]))]\n",
    "    f_indices = [j for j in range(len(function_values)) ] \n",
    "\n",
    "    ax[0].set_title(title[0])\n",
    "\n",
    "    fig.set_figwidth(15) \n",
    "    for k in range(len(convergences)):\n",
    "        ax[0].plot(c_indices,convergences[k],label='q = ' + str(k+1))\n",
    "\n",
    "    ax[0].legend(loc = \"upper left\")\n",
    "\n",
    "    ax[0].set_xlabel('t')\n",
    "    ax[1].set_title(title[1])\n",
    "    ax[1].plot(f_indices,function_values)\n",
    "    ax[1].set_xlabel('t')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_point = np.array([[1.2],[1.2]])\n",
    "iterations_steep_p1, iterates_steep_p1, function_values_steep_p1, step_sizes_steep_p1 = steepest_descent_rosenbrock(initial_point,no_stopping_criterion=True)\n",
    "initial_point = np.array([[-1.2],[1.0]])\n",
    "iterations_steep_p2, iterates_steep_p2, function_values_steep_p2, step_sizes_steep_p2 = steepest_descent_rosenbrock(initial_point,no_stopping_criterion=True)\n",
    "\n",
    "\n",
    "convergences_p1 = []\n",
    "convergences_p2 = []\n",
    "\n",
    "\n",
    "for i in range(1,4):\n",
    "    convergences_p1.append(convergence(iterates_steep_p1,np.array([[1.0],[1.0]]),i))\n",
    "    convergences_p2.append(convergence(iterates_steep_p2,np.array([[1.0],[1.0]]),i))\n",
    "\n",
    "plot_convergence_and_fnvalues(function_values_steep_p1,convergences_p1,['Steepest Descent Convergence (Initial Point = [1.2,1.2])', 'Iterates Objective function values (Initial Point = [1.2,1.2])'])\n",
    "plot_convergence_and_fnvalues(function_values_steep_p2,convergences_p2,['Steepest Descent Convergence (Initial Point = [-1.2,1.0])', 'Iterates Objective function values (Initial Point = [-1.2,1.0])'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate of Convergence of Steepest Descent Method\n",
    "\n",
    "- We can see that the convergence value  for q=1 is almost a constant that we can bound it by some positive constant $M$\n",
    "- We can see a positive slope in the convergence function for $q=2$ and $q=3$ towards the end.\n",
    "- The rate of convergence of the steepest descent method is SuperLinear for both the initial points.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_point = np.array([[1.2],[1.2]])\n",
    "iterations_newton_p1, iterates_newton_p1, function_values_newton_p1, step_sizes_newton_p1 = newton_rosenbrock(initial_point,no_stopping_criterion=False)\n",
    "initial_point = np.array([[-1.2],[1.0]])\n",
    "iterations_newton_p2, iterates_newton_p2, function_values_newton_p2, step_sizes_newton_p2 = newton_rosenbrock(initial_point,no_stopping_criterion=False)\n",
    "\n",
    "\n",
    "convergences_p1 = []\n",
    "convergences_p2 = []\n",
    "\n",
    "\n",
    "for i in range(1,4):\n",
    " \n",
    "    convergences_p1.append(convergence(iterates_newton_p1,np.array([[1.0],[1.0]]),i))\n",
    "    convergences_p2.append(convergence(iterates_newton_p2,np.array([[1.0],[1.0]]),i))\n",
    "\n",
    "plot_convergence_and_fnvalues(function_values_newton_p1[:20],convergences_p1,['Newtons Method Convergence (Initial Point = [1.2,1.2])', 'Iterates Objective function values (Initial Point = [1.2,1.2])'])\n",
    "plot_convergence_and_fnvalues(function_values_newton_p2[:150],convergences_p2,['Newtons Descent Convergence (Initial Point = [-1.2,1.0])', 'Iterates Objective function values (Initial Point = [-1.2,1.0])'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rate of Convergence of Newtons Method\n",
    "\n",
    "- We can see that the convergence value  for both  q=2 and q=1 is almost a constant that we can bound it by some positive constant $M$\n",
    "- We can see a huge convergence value for $q=3$ (1e7) that makes it difficult to find a bound for the convergence value as  \n",
    "$k \\rightarrow \\infty$\n",
    "- The rate of convergence of the steepest descent method is Quadratic for both the initial points. (A sequence that converges quadratic also converges superlinearly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steepest_descent_rosenbrock_scipy(point):\n",
    "    current_point = point.copy()\n",
    "    previous_point = point.copy()\n",
    "    iterations =0\n",
    "    iterates = []\n",
    "    while True:\n",
    "        iterations+=1\n",
    "        iterates.append(current_point)\n",
    "        previous_point = current_point.copy()\n",
    "        current_gradient = approx_fprime(current_point.squeeze(),rosenbrock)  # Built in function for calculating the gradient\n",
    "        current_gradient = np.expand_dims(current_gradient,axis=1)\n",
    "        #current_gradient = np.expand_dims(rosenbrock_jacobian(current_point.squeeze()),1)\n",
    "        \n",
    "        # Built in Function for calculating the step length which satisifies the Wolfe Conditions\n",
    "        step_length = line_search(rosenbrock,rosenbrock_jacobian,current_point.squeeze(),-get_unit_vector(current_gradient).squeeze())\n",
    "        if step_length[0] == None:\n",
    "            break\n",
    "        current_point += step_length[0]*(-get_unit_vector(current_gradient))\n",
    "    return iterations,iterates[-1]\n",
    "\n",
    "\n",
    "def newtons_method_scipy(point):\n",
    "    current_point = point.copy()\n",
    "    ans = minimize(rosenbrock,current_point,method='Newton-CG',jac=rosenbrock_jacobian,hess=rosenbrock_hessian)\n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "def compare_steepest(point):\n",
    "    init_point = point.copy()\n",
    "    t1 = datetime.datetime.now()\n",
    "    it1,f1 = steepest_descent_rosenbrock(init_point,return_extra_info=False)\n",
    "    t2 = datetime.datetime.now()\n",
    "    print(\"Time taken Steepest Descent (Own Implementation) =\" + str(t2-t1))\n",
    "    print(\"Iterations taken = \" + str(it1))\n",
    "    print(\"Final Value after convergence\" + str(f1))\n",
    "\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    t1 = datetime.datetime.now()\n",
    "    it2,f2 = steepest_descent_rosenbrock_scipy(init_point)\n",
    "    t2 = datetime.datetime.now()\n",
    "    print(\"Time taken Steepest Descent (BuiltIn Implementation) =\" + str(t2-t1))\n",
    "    print(\"Iterations taken = \" + str(it2))\n",
    "    print(\"Final Value after convergence\" + str(f2))\n",
    "\n",
    "def compare_newton(point):\n",
    "\n",
    "    init_point = point.copy()\n",
    "    t1 = datetime.datetime.now()\n",
    "    it1,f1 = newton_rosenbrock(init_point,return_extra_info=False)\n",
    "    t2 = datetime.datetime.now()\n",
    "    print(\"Time taken Newtons Method (Own Implementation) =\" + str(t2-t1))\n",
    "    print(\"Iterations taken = \" + str(it1))\n",
    "    print(\"Final Value after convergence\" + str(f1))\n",
    "\n",
    "    print(\"-----------------------------------------------\")\n",
    "\n",
    "    t1 = datetime.datetime.now()\n",
    "    res = minimize(rosenbrock,init_point,method='Newton-CG',jac=rosenbrock_jacobian,hess=rosenbrock_hessian)\n",
    "    it2,f2,jac = res['nit'],res['x'] ,res['jac']\n",
    "    t2 = datetime.datetime.now()\n",
    "    print(\"Time taken Newtons Method (BuiltIn Implementation) =\" + str(t2-t1))\n",
    "    print(\"Iterations taken = \" + str(it2))\n",
    "    print(\"Final Value after convergence\" + str(f2))\n",
    "    print(\"Jacobian\" + str(jac))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Point (1.2,1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "compare_steepest(np.array([[1.2],[1.2]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_newton(np.array([[1.2],[1.2]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Point (-1.2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_steepest(np.array([[-1.2],[1.0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_newton(np.array([[-1.2],[1.0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tabular Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Steepest Descent\n",
    "| Initial Point     | Iterations(Steepest Descent- Own) | Time(seconds)(Steepest Descent -Own) | Minima(Steepest Descent - Own) | Iterations(Steepest Descent- Scipy) | Time (Steepest Descent -Scipy) | Minima(Steepest Descent - Scipy) |\n",
    "| ----------- | ----------- |-----| ----- | ----- | ----- | ----- | \n",
    "| (1.2,1.2)      | 8332      |  2.603 s  | (1.00018,1.00036) | 8 | 0.001796  s| (1.030602,1.06217019) |\n",
    "| (-1.2,1.0)  | 8843       |  2.642 s  | (0.9982,0.9999) | 137 | 0.0195415 s  | (1.01904608,1.03840753) |\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "#### Newtons Method\n",
    "\n",
    "| Initial Point     | Iterations(Newtons Method - Own) | Time (Newtons Method -Own) | Minima(Newtons Method - Own) | Iterations(Newtons Method - Scipy) | Time (Newtons Method -Scipy) | Minima(Newtons Method - Scipy) |\n",
    "| ----------- | ----------- |-----| ----- | ----- | ----- | ----- | \n",
    "| (1.2,1.2)      | 7      |   0.001403 s  | (1.0,1.0) | 12 | 0.001637 s | $(0.99999998 , 0.99999997) $|\n",
    "| (-1.2,1.0)  | 20       |  0.001973 s  | (1.0,1.0) | 83 | 0.007742 s  | $(0.9999826 ,  0.99996514)$ |\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jacobian(iterates ,init_point = '[1.2,1.2]'):\n",
    "    jacobian_values = []\n",
    "\n",
    "    for iterate in iterates:\n",
    "\n",
    "        jacobian_values.append(rosenbrock_jacobian(iterate.squeeze()))\n",
    "\n",
    "    j_df = pd.DataFrame({'Jacobian' + init_point : jacobian_values})\n",
    "\n",
    "    return j_df\n",
    "\n",
    "df1 = calculate_jacobian(iterates_newton_p1)\n",
    "df2 = calculate_jacobian(iterates_newton_p2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Jacobian at every point Starting point = [1.2,1.2]\")\n",
    "print(df1)\n",
    "print(\"\")\n",
    "print(\"Final Jacobian computed by BuiltIn Function; Starting point = [1.2,1.2] = [4.62401128e-06 -2.32450401e-06] \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Jacobian at every point Starting point = [-1.2,1.0]\")\n",
    "print(df2)\n",
    "print(\"\")\n",
    "print(\"Final Jacobian computed by BuiltIn Function; Starting point = [-1.2,1.0] = [ 0.00510412 -0.00256439] \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparisons\n",
    "\n",
    "-  We can see differences in both the run time and results of builtin functions and custom made functions\n",
    "\n",
    "- The minima computed by the builtin and custom implementations are almost similar.\n",
    "\n",
    "-  Steepest Descent done through builtin functions calculates the best step length to take better than the custom implementation. This explains the low run time and number of iterations taken to converge. \n",
    "\n",
    "- The Newtons implementation on the other hand is slightly faster on using the custom implementation rather than builtin library functions.\n",
    "\n",
    "- The builtin Newtons method does more iterations than the custom implementation which might be due to less amounts of update performed by the builtin function at each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d1b0110cf1cb03549be737c5657a86ea4daeeb81469a7991ed915d907f3e629c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
